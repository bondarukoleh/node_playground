Deployment
Once you’ve built and tested a Node application, you’ll want to release it. Popular
PaaS (platform as a service) providers like Heroku and Nodejitsu make deployment
simple, but you can also deploy to private servers.

Easy to use cloud deployment: Nodejitsu, Heroku, and Windows Azure.
Nvm - is a good node version manager easy to update node on UNIX machines.
npm-windows-upgrade tool should be ok, but you can just download and reinstall it to update node on Windows.
Also we can use apache and nginx.

You can run app on your own machine, but you need to rebind traffic from 80 port to port
where your app is. You can do it in Linux via iptables. BUT downside of this technique is 
that it maps traffic to any process that’s listening to that port.
Alternative is to change node binaries to bind app to some specific port, don't get the
advantage of this.

Keeping Node processes running
There are two main ways to keep a Node program running: service supervision or a
Node program that manages other Node programs.
The first method is a generic, operating system–specific technique. Runit for example
supports service supervision, which means it detects when a process stops running and
tries to restart it. Another daemon manager is Upstart.

Node process managers work by using a small program that ensures another program
runs continuously. This program is simple and therefore less likely to crash than
a more complex web application. One of the most popular modules for this is forever
(https://www.npmjs.org/package/forever), which can be used as a command-line program
or programmatically.
Most people use it through the command-line interface. The basic usage is
forever start app.js, where app.js is your web application. It has lots of options
beyond this, though: it can manage log files and even wrap your program so it
behaves like a daemon.

Deploying socket service.
Deploying applications that use WebSockets can bring a set of unique requirements.
It can be more difficult with PaaS providers, because they can kill requests that
last for more than a certain number of seconds.

HTTP is essentially a stateless protocol, which means all interactions between a
server and a client can be modeled with requests and responses that hold all of the
required state. This level of encapsulation has led to the design of modern client/
server web applications.
The downside of this is that the underlying protocol doesn’t support long-running
full-duplex connections. There’s a wide class of applications that are built on TCP connections
of this type; video streaming and conferencing, real-time messaging, and
games are prominent examples. As web browsers have evolved to support richer, more
sophisticated applications, we’re naturally left trying to simulate these types of applications
using HTTP.
The WebSocket protocol was developed to support long-lived TCP-like connections.
It works by using a standard HTTP handshake where the client establishes
whether the server supports WebSockets. The mechanism for this is a new header
called Upgrade. As HTTP clients and servers are typically bombarded with a variety of
nonstandard headers, servers that don’t support Upgrade should be fine—the client
will just have to fall back to old-fashioned HTTP polling.
Because servers have to handle WebSocket connections so differently, it makes sense
to effectively run two servers. In a Node program, we typically have an http.listen for
our standard HTTP requests, and another “internal” WebSocket server.
We saw how to use nginx with Node. The example used proxies
to pass requests from nginx to your Node process, which meant the Node process
could bind to a different port to 80. By using the same technique, you can make nginx
support WebSockets.

So the main idea - is to use proxy servers that will decide where to sent the client
request, either to http or socket server.
Also with proxy - we cat setup a small round-robin load balancing which is pretty cool. 

Another approach that we haven’t discussed yet is to put your Node applications
behind a lightweight Node program that acts as a proxy itself. This is actually used
behind the scenes by PaaS providers like Nodejitsu.
Selecting the right server architecture is just the first step to successfully deploying
a Node application. You should also consider performance and scalability.

Caching and scaling
Browsers cache content locally, and can look at the cache to determine if a
resource needs to be downloaded. This process is controlled by HTTP cache headers and
conditional requests.
The main two headers are Cache-Control and Expires. The Cache-Control
header allows the server to specify a directive that controls how a resource is cached.
The basic directives are as follows:
■ public—Allow caching in the browser and any intermediate proxies between
the browser and server.
■ private—Only allow the browser to cache the resource.
■ no-store—Don’t cache the resource (but some clients still cache under certain
conditions).

The Expires header tells the browser when to replace the local resource. The date
should be in the RFC 1123 format: Fri, 03 Apr 2014 19:06 BST. The HTTP/1.1 specification
notes that dates over a year shouldn’t be used, so don’t set dates too far into the
future because the behavior is undefined.

Conditional caching is great for large assets that may change, like images, because it’s
much cheaper to make a GET request to find out if a resource should be downloaded
again. This is known as a time-based conditional request. There are also content-based
conditional requests, where a digest of the resource is used to see if a resource has changed.
So the idea is to check is time is run out and we need to download new resource, 
or is the file is changed - and we nned to download it again.
Content-based conditional requests work using ETags. ETag is short for entity tag,
and allows servers to validate resources in a cache based on their content.

For example, we set the etag to some resource, and give it value of resource hash.
If client requests resource once again, we can grap hash from request and compare it 
with current resource state - if it's same - no need to download resource once again.

Node proxy for routing and scalling
Caching is an elegant way of improving performance because it effectively allows
you to reduce traffic by making clients do a bit more work. Another option is to use a
Node-based HTTP proxy to route between a cluster of processes or servers.
You want to use a pure Node solution to host multiple applications, or scale an
application - Use a proxy server module like Nodejitsu’s http-proxy.
With it we can true easily setup a proxy, and simple load balancer.
Cluster
You want to improve your application’s response time, or increase its resiliency - 
use cluster module.
We’re trained to think of Node programs as single-threaded because JavaScript platforms are
conceptually single-threaded, but behind the scenes, Node’s libraries like libuv will use
threads to provide asynchronous APIs. That gives us the eventbased programming style without
having to worry about the complexity of threads.
require('os').cpus() - how many cores you've got

  The cluster module provides a way of running multiple worker processes that
share access to underlying file handles and sockets. That means you can wrap a Node
application with a master process that works workers. Workers don’t need access to
shared state if you’re doing things like accessing user sessions in a database; all the
workers will have access to the database connection, so you shouldn’t need to set up
any communication between workers.
  If one of your workers encounters an error and the process ends, then
you’ll want to restart it. The cool thing about this is any other active workers can still
serve requests, so clustering will not only improve request latency but also potentially
uptime as well.
Communication between workers - great, but If you’re running an application on separate servers,
how can Node processes communicate?
  One simple answer might be HTTP — you could build an internal REST API for communication.
You could even use WebSockets if messages need faster responses. We can use RabbitMQ - simple 
messager to communicate between servers. There are several implementations of RabbitMQ clients
on npm—we used amqplib. There are also competitors to RabbitMQ—zeromq is a highly focused and
simple alternative.
  Another option is to use a hosted publish/subscribe service. One example of this is
Pusher (http://pusher.com/), which uses WebSockets to help scale applications. The
advantage of this approach is that Pusher can message anything, including mobile clients.
Rather than restricting messaging to your Node programs, you can create message
channels that web, mobile, and even desktop clients can subscribe to.

It is usefull to benchmark test of cluster. Load test it.
There is a tool "ab" in apache. 

  Also very cool stuff - Load balancing with an additional proxy server (nginx, apache) doesn’t
have the same kind of interprocess communication options that cluster has—you can use
process.send and cluster.workers[id].on('message', fn) to communicate between workers.
  But proxies with dedicated load-balancing features have a wider choice of loadbalancing
algorithms.

Package optimization
npm prune and npm shrinkwrap.
  The npm prune command removes packages that are no longer listed in your package.json,
but it also applies to the dependencies themselves, so it can sometimes dramatically reduce
your application’s storage footprint. You should also consider using npm prune --production to
remove devDependencies from production releases.
  Shrinkwrap analyze packeges and make a map of them in generated npm-shrinkwrap.json. We can 
add it to repository, and npm should use it to download accurate version of packages.
Difference between package-lock.json and npm-shrinkwrap.json:
  - using a package-lock.json to record exactly which versions of dependencies you installed,
  but allowing people installing your package to use any version of the dependencies that is
  compatible with the version ranges dictated by your package.json
  - using an npm-shrinkwrap.json to guarantee that everyone who installs your package gets exactly
  the same version of all dependencies
shrinkwrap - is more straight.  

Logging and logging services
You want to log messages from a Node application on your own server, or on a PaaS provider.
Either redirect logs to files and use logrotate, or use a third-party logging service.

npm start 1> application.log 2> errors.log

### Jenkins notes
* Jenkins pwd in job scripts - is in jenkins/workspace/your_job_name
* On Linux - to run some programs jenkins needs to have sudo rights. To provide it:
$> sudo gedit /etc/sudoers
Add a row in file: jenkins ALL=(ALL) NOPASSWD: ALL
but that's not a great idea.
